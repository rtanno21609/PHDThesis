
\section{Discussion and Conclusion}
In this chapter, we introduced Adaptive Neural Trees (ANTs), a simple way to marry the architecture learning, conditional computation and hierarchical clustering of decision trees (DTs) with the hierarchical representation learning and gradient descent optimization of deep neural networks (DNNs). Our proposed training algorithm optimises both the parameters and architectures of ANTs through progressive growth, tuning them to the size and complexity of the training dataset. Together, these properties make ANTs a generalisation of previous work attempting to unite NNs and DTs. Finally, we validated the claimed benefits of ANTs for regression (SARCOS dataset) and classification (MNIST \& CIFAR10 datasets), whilst still achieving high performance. 

Future work will aim to scale up ANTs to larger, higher dimensional datasets. This requires training potentially wider and deeper ANTs, which necessitates a more effective and efficient optimisation algorithm. Firstly, the current growth procedure is greedy and suboptimal. The sub-optimality of the local decisions only gets worse as the tree gets more complex. In the future, we will look into different ways to alleviate this issue such as (1)  more global optimisation during growth phase, (2) merging of branches \cite{shotton2013decision} or (3) training in entangled settings as already done in decision tree research \cite{montillo2011entangled}. Secondly, for large trees, the global refinement phase would be computationally expensive since the associated computation involves every part of the tree structure. One potential solution to this is to employ a Monte-Carlo approximation of the training by stochastically traversing the tree, while using gradient estimators for learning the parameters of the routers such as REINFORCE and straight-through (ST) estimators \cite{bengio2013} or more modern methods such as Gumbel ST estimator \cite{jang2016categorical}, REBAIR and RELAX \textcolor{red}{(references)}. We have also limited ourselves to relatively simple NN components in the design of the ANT's primitive modules. As future work, we aim to extend ANTs to use more recent advances in deep learning, particularly residual learning \cite{he2016deep} and dense connections \cite{huang2017densely}, as these have enabled the successful optimisation of more performant NNs. 

\textcolor{red}{Shouldn't we also mention that the architecture is not very stable at the moment. We can chalk this up to the optimisation. May be try to integrate this point in the above paragraph. }


Another important challenge is to explore the value of the learned hierarchical structures of ANTs  in terms of “interpretability” (or algorithmic transparency). I believe that the tree-shaped hierarchy provides a new means to understand its internal decision making process---for instance, given an image where the ANT fail to classify, you could compare against a large number of correctly predicted examples, and potentially localise a point of routing failure where the ANT makes the wrong decision. On the other hand, such localisation of failures is more difficult in conventional CNNs with a single fully distributed representation. In addition, such hierarchical and decomposed interpretations are complementary to existing visualisation techniques such as saliency based attribution methods \textcolor{red}{(references)}. For example, providing saliency maps of the series of routing decisions may potentially provide a layer of transparency into the decisions made in the raw feature space. 

% Currently, the predictive distributions from all existing paths on the tree are computed during training, which can become expensive for large models, especially during the fine-tuning phase based on global optimisation. To combat this issue, we experimented with various forms of MC approximations of the default training algorithm where each sample stochastically traverses the tree-structure, only engaging parameters on the selected root-to-leaf path.

% In contrast with the default soft routing decisions, however, the MC approximation operates on stochastic hard decisions i.e. samples from Bernouli random variables given by the routers, rendering backpropgation non-usable. We therefore used various gradient estimators, such as REINFORCE and straight-through (ST) estimator (Bengio, 2013, see option -r_sto and -no_r_soft) and Gumbel ST estimator (Eric et al., 2016, see option --router_gumbel). Although more efficient, we have observed aggravated quality of local optimisation, especially at deeper levels. REINFORCE is unbiased, but suffer from high variance while ST estimators are low-variance but biased. It may be worth trying in the future more advanced gradient estimators (unbiased and lower variance) such as RELAX.

%As future work, we aim to extend ANTs to use more recent advances in deep learning, particularly residual learning \cite{he2016deep} and dense connections \cite{huang2017densely}, as these have enabled the successful optimisation of more performant NNs. 

%Through a series of experiments on object classification datasets, we demonstrated that ANTs can achieve high accuracy while retaining most of the benefits of decision trees, such as hierarchical clustering and conditional computation. ANTs can also be applied to small datasets, as the training procedure performs model selection automatically.

%In the end, this paper makes steps towards answering the question of whether it is better to partition data, or learn yet another level of features. In introducing a new model and training algorithm, we limited ourselves to relatively simple NN components. However, recent work has shown the importance of skip-connections in optimising very deep neural networks \cite{he2016deep,huang2017densely}. In future work we plan to scale up and otherwise improve upon ANTs by exploring the use of residual \cite{he2016deep} and dense connections \cite{huang2017densely}, which we hope will allow us to bridge the performance gap between our current architectures and these state-of-the-art networks.