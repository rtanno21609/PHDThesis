
\section{Conclusion}
\vspace{-2mm}
We introduced Adaptive Neural Trees (ANTs), a holistic way to marry the architecture learning, conditional computation and hierarchical clustering of decision trees (DTs) with the hierarchical representation learning and gradient descent optimization of deep neural networks (DNNs). %In this work, we introduced adaptive neural trees (ANTs), a novel form of decision trees (DTs) which use neural networks (NNs) in both its edges and nodes, combining the abilities of representation learning with hierarchical data partitioning. 
Our proposed training algorithm optimises both the parameters and architectures of ANTs through progressive growth, tuning them to the size and complexity of the training dataset. Together, these properties make ANTs a generalisation of previous work attempting to unite NNs and DTs. Finally, we validated the claimed benefits of ANTs for regression (SARCOS dataset) and classification (MNIST \& CIFAR10 datasets), whilst still achieving high performance. 



% Future work will aim to scale up the algorithm to b

% Currently, the predictive distributions from all existing paths on the tree are computed during training, which can become expensive for large models, especially during the fine-tuning phase based on global optimisation. To combat this issue, we experimented with various forms of MC approximations of the default training algorithm where each sample stochastically traverses the tree-structure, only engaging parameters on the selected root-to-leaf path. In contrast with the default soft routing decisions, however, the MC approximation operates on stochastic hard decisions i.e. samples from Bernouli random variables given by the routers, rendering backpropgation non-usable. We therefore used various gradient estimators, such as REINFORCE and straight-through (ST) estimator (Bengio, 2013, see option -r_sto and -no_r_soft) and Gumbel ST estimator (Eric et al., 2016, see option --router_gumbel). Although more efficient, we have observed aggravated quality of local optimisation, especially at deeper levels. REINFORCE is unbiased, but suffer from high variance while ST estimators are low-variance but biased. It may be worth trying in the future more advanced gradient estimators (unbiased and lower variance) such as RELAX.

%As future work, we aim to extend ANTs to use more recent advances in deep learning, particularly residual learning \cite{he2016deep} and dense connections \cite{huang2017densely}, as these have enabled the successful optimisation of more performant NNs. 

%Through a series of experiments on object classification datasets, we demonstrated that ANTs can achieve high accuracy while retaining most of the benefits of decision trees, such as hierarchical clustering and conditional computation. ANTs can also be applied to small datasets, as the training procedure performs model selection automatically.

%In the end, this paper makes steps towards answering the question of whether it is better to partition data, or learn yet another level of features. In introducing a new model and training algorithm, we limited ourselves to relatively simple NN components. However, recent work has shown the importance of skip-connections in optimising very deep neural networks \cite{he2016deep,huang2017densely}. In future work we plan to scale up and otherwise improve upon ANTs by exploring the use of residual \cite{he2016deep} and dense connections \cite{huang2017densely}, which we hope will allow us to bridge the performance gap between our current architectures and these state-of-the-art networks.