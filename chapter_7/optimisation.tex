\section{Optimisation}\label{sec:learning}
Training of an ANT proceeds in two stages: 1) \textit{growth phase} during which the model architecture is learned based on \textit{local} optimisation, and 2) \textit{refinement phase} which further tunes the parameters of the model discovered in the first phase based on \textit{global} optimisation. We include a pseudocode of the training algorithm in Supp.~Sec.~A. %First, we describe a backpropagation-based algorithm \cite{rumelhart1986learning} for learning parameters in $\mathbb{O}$ given a fixed topology $\mathbb{T}$, which forms the optimisation backbone for both the growth and refinement phases. We then describe a method for learning the model topology, $\mathbb{T}$, and fine-tune it. 

% --------- optimising the parameters in operational modules ------------
\subsection{Loss function: optimising parameters of \texorpdfstring{$\mathbb{O}$}{O}}\label{sec:graddescent}
% \subsection{Loss function: optimising parameters for fixed architecture \texorpdfstring{}{O}}\label{sec:graddescent}
For both phases, we use the negative log-likelihood (NLL) as the common objective function to minimise: 

$$-\text{log }p(\mathbf{Y}|\mathbf{X}, \Theta) = -\sum_{n=1}^N\text{log }(\sum_{l=1}^{L} \pi_{l}^{\boldsymbol{\theta}, \boldsymbol{\psi}}(\mathbf{x}^{(n)})\,
p_{l}^{\boldsymbol{\phi}, \boldsymbol{\psi}}(\mathbf{y}^{(n)}))$$ where $\mathbf{X} = \{\mathbf{x}^{(1)}, ..., \mathbf{x}^{(N)}\}$, $\mathbf{Y} = \{\mathbf{y}^{(1)}, ..., \mathbf{y}^{(N)}\}$ denote the training inputs and targets. As all component modules (routers, transformers and solvers) are differentiable with respect to their parameters $\Theta = (\boldsymbol{\theta}, \boldsymbol{\psi}, \boldsymbol{\phi})$, we can use gradient-based optimisation. Given an ANT with fixed topology  $\mathbb{T}$, we use backpropagation \cite{rumelhart1986learning} for gradient computation and use gradient descent to minimise the NLL for learning the parameters.

%Given a fixed ANT $(\mathbb{T},\mathbb{O})$ and training inputs/targets $\mathbf{X} = \{\mathbf{x}^{(1)}, ..., \mathbf{x}^{(N)}\}$, $\mathbf{Y} = \{\mathbf{y}^{(1)}, ..., \mathbf{y}^{(N)}\}$, we jointly optimise the hierarchical grouping of data to paths on the tree and the associated expert NNs by maximizing the likelihood $p(\mathbf{Y}|\mathbf{X}, \boldsymbol{\theta}, \boldsymbol{\psi}, \boldsymbol{\phi}) = \prod_{n=1}^N\sum_{l=1}^{L} \pi_{l}^{\boldsymbol{\theta}, \boldsymbol{\psi}}(\mathbf{x})\,
%p_{l}^{\boldsymbol{\phi}, \boldsymbol{\psi}}(\mathbf{y}) $. As all the component modules in $\mathbb{O}$ are assumed differentiable with respect to parameters $\Theta = (\boldsymbol{\theta}, \boldsymbol{\psi}, \boldsymbol{\phi})$,  we can use gradient-based optimisation. We use backpropagation for gradient computation and use gradient descent to minimise the negative log-likelihood (NLL): $-\log p(\mathbf{Y}|\mathbf{X},\boldsymbol{\theta}, \boldsymbol{\psi}, \boldsymbol{\phi})$. We employ this scheme for learning parameters in $\mathbb{O}$ in both growth and refinement phases, as explained next. 

% The generalised M-step generates a new estimate of the parameters by taking a gradient ascent on the objective function eq.\eqref{eq:emobj}: $\Theta^{(t+1)} := \Theta^{(t)} + \lambda \frac{\partial}{\partial\Theta}\mathcal{Q}(\Theta|\Theta^{(t)})$ where the gradient is given by the weighted average of component log likelihoods prorated by the previously computed assignment probabilities
% $\frac{\partial}{\partial\Theta}\mathcal{Q}(\Theta|\Theta^{(t)}) = \sum_{n=1}^N \sum_{l=1}^{L} \gamma^{\Theta^{(t)}}(\mathbf{x}^{(n)},\mathbf{y}^{(n)})\, \frac{\partial}{\partial\Theta}\log \pi_{l}^{\boldsymbol{\theta}, \boldsymbol{\psi}}(\mathbf{x^{(n)}})\,
% p_{l}^{\boldsymbol{\phi}, \boldsymbol{\psi}}(\mathbf{y^{(n)}})$.

% \subsubsection{Monte-Carlo approximation of EM}
% The number of leaf nodes can be very large for a deep ANT. The EM-based optimisation requires computation along all possible paths in the given tree structure, incuring an undesirable trade-off between model complexity and training computation. To combat this we propose an Monte-Carlo estimation of the EM objective $\mathcal{Q}(\Theta|\Theta^{(t)})$. 

% In particular, for each input $\mathbf{x}$, we draw multiple samples of destination leaf nodes $\mathcal{N}(\mathbf{x})=\{l_1, ..., l_T\}, \, l_i \sim \pi_{l}^{\boldsymbol{\theta}, \boldsymbol{\psi}}(\mathbf{x})$ by stochastically traversing the tree multiple times and approximate the reaching probability $\pi_{l}^{\boldsymbol{\theta}, \boldsymbol{\psi}}(\mathbf{x})$ for respective leaf nodes by the normalised counting function:
% \begin{equation}
% \widehat{\pi}_{l}^{\boldsymbol{\theta}, \boldsymbol{\psi}}(\mathbf{x}) := \frac{1}{|\mathcal{N}(\mathbf{x})|} \sum_{\tilde{l}\in \mathcal{N}(\mathbf{x}) }\mathbbm{1}[l=\tilde{l}]
% \end{equation}
% We can now form the MC-estimates, $\widehat{\gamma}^{\Theta}(\mathbf{x},\mathbf{y})$ and $\widehat{\mathcal{Q}}(\Theta|\Theta^{(t)})$ of the assigment probability and expected complete-data loglikeilihood by replacing $\pi_{l}^{\boldsymbol{\theta}, \boldsymbol{\psi}}(\mathbf{x})$ terms in eq. \eqref{eq:labelprob} and \eqref{eq:emobj} with $\widehat{\pi}_{l}^{\boldsymbol{\theta}, \boldsymbol{\psi}}(\mathbf{x})$. If a particular computational path is not sampled i.e. $\widehat{\pi}_{l}^{\boldsymbol{\theta}, \boldsymbol{\psi}}(\mathbf{x})=0$, then we do not need to compute the conditional likelihood term $p_{l}^{\boldsymbol{\phi}, \boldsymbol{\psi}}(\mathbf{y})$ at the corresponding leaf node. Therefore, so long as the number of samples $|\mathcal{N}(\mathbf{x})|$ is smaller than the number of leaf nodes, the MC estimates requires less memory and computation than the proposed EM-based iterative optimisation.

% In contrast with the ``soft" assignement of data to different branches in computing $\mathcal{Q}(\Theta|\Theta^{(t)})$, the MC approximation based on sampled computation paths in a tree entails taking a set of sequences of stochastic ``hard" routing decisions where each datum is strictly guided to one branch (left or right) at a time. The need for sampling discrete (binary) decisions means that $\widehat{\mathcal{Q}}(\Theta|\Theta^{(t)})$ is no longer differentiable with respect to the parameters of the router modules $\boldsymbol{\theta}$ (the mean of Bernouli distributions) and so the M-step needs a modification. To this end, we employ a biased path-derivative estimator for Bernouli variables  (``straight-through" estimator) \cite{bengio2013deep} to approximate its expected gradiants. 

% In our experiments, we observed that the number of sampled paths $|\mathcal{N}(\mathbf{x})|$ can be set to $1$ as long as the minibatch size was sufficiently large (e.g. $256$). This simplification renders the E-step redundant (setting $\widehat{\gamma}^{\Theta^{(t)}}(\mathbf{x},\mathbf{y})=1$ for all data points) and reduces the M-step to gradient ascent on the sample conditional log-likelihood
% \begin{equation}\label{eq:emobj}
% \widehat{\mathcal{Q}}(\Theta|\Theta^{(t)}) = \sum_{n=1}^N \log\,p_{l(\mathbf{x^{n}})}^{\boldsymbol{\phi}, \boldsymbol{\psi}}(\mathbf{y}^{(n)}) 
% \end{equation}
% where $l(\mathbf{x^{n}})$ is the sampled leaf node for input $\mathbf{x^{n}}$. We will show in section \ref{sec:experiments} that this aggresive approximation leads to competitive performance in accuracy while significantly reducing the space and time complexity of the original EM algorithm. 
\subsection{Growth phase: learning architecture \texorpdfstring{$\mathbb{T}$}{T}}
We next describe our proposed method for growing the tree $\mathbb{T}$ to an architecture of adequate complexity for the given training data. Starting from the root, we choose one of the leaf nodes in breadth-first order and incrementally modify the architecture by adding computational modules to it. In particular, we evaluate $3$ choices (Fig. \ref{fig:hierarchy} (Right)) at each leaf node; (1)``split data" extends the current model by splitting the node with an addition of a new router; (2) ``deepen transform" increases the depth of the incoming edge by adding a new transformer; (3) ``keep" retains the current model. We then locally optimise the parameters of the newly added modules in the architectures of (1) and (2) by minimising NLL via gradient descent, while fixing the parameters of the previous part of the computational graph. Lastly, we select the model with the lowest validation NLL if it improves on the previously observed lowest NLL, otherwise we execute (3). This process is repeated to all new nodes level-by-level until no more ``split data" or ``deepen transform" operations pass the validation test. 

The rationale for evaluating the two choices is to give the model a freedom to choose the most effective option between ``going deeper'' or splitting the data space. Splitting a node is equivalent to a soft partitioning of the feature space of incoming data, and gives birth to two new leaf nodes (left and right children solvers). In this case, the added transformer modules on the two branches are identity functions. Deepening an edge on the other hand seeks to learn richer representation via an extra nonlinear transformation, and replaces the old solver with a new one. Local optimisation is efficient in time and space; gradients only need to be computed for the parameters of the new parts of the architecture, reducing computation, while forward activations prior to the new parts do not need to be stored in memory, saving space. 

%It is noteworthy that we perform the local optimisation of each `new' architecture for relatively small number of epochs (e.g. up to 20 epochs on MNIST/CIFAR-10). This is motivated by the observation that the performance of a network early in training provides a meaningful indication of performance after convergence \cite{li2017hyperband,brock2017smash}.  We observed that this approach is not only more efficient but also achieves a more accurate model than the standard greedy protocol used in decision tree training where local optimisation is performed until convergence \cite{criminisi2013decision,breiman2001random}. This is presumably because the model is less likely to overfit locally.

\subsection{Refinement phase: global tuning of \texorpdfstring{$\mathbb{O}$}{O}}
Once the model topology is determined in the growth phase, we finish by performing global optimisation to refine the parameters of the model, now with a fixed architecture. This time, we perform gradient descent on the NLL with respect to the parameters of all modules in the graph, jointly optimising the hierarchical grouping of data to paths on the tree and the associated expert NNs. The refinement phase can correct suboptimal decisions made during the local optimisation of the growth phase, and empirically improves the generalisation error (see Sec. \ref{sec:refinement}).