\section{Related work}\label{sec:relatedwork}

Our work is primarily related to research into combining DTs and NNs. Here we explain how ANTs subsume a large body of such prior work as specific cases and address their limitations. We also include additional reviews of work in conditional computation, neural architecture search, and an early form of feature learning with DTs based on cascading. 

\paragraph{Combining Decisions Trees and Neural Networks:}
The very first soft decision tree (SDT) introduced in \cite{suarez1999globally} is a specific case where in our terminology the routers are axis-aligned features, the transformers are identity functions, and the routers are static distributions over classes or linear functions. The hierarchical mixture of experts (HMEs) proposed by \cite{jordan1994hierarchical} is a variant of SDTs whose routers are linear classifiers and the tree structure is fixed; \cite{leon2015policy} recently proposed a more computationally efficient training method that is able to directly optimise hard-partitioning by differentiating through stochastic gradient estimators. More modern SDTs in \cite{rota2014neural,laptev2014convolutional,frosst2017distilling} used multilayer perceptrons (MLPs) or convolutional layers in the routers to learn more complex partitionings of the input space. However, the simplicity of identity transformers used in these methods means that input data is never transformed and thus each path on the tree does not perform representation learning, limiting their performance.

More recent work suggested that integrating non-linear transformations of data into DTs would enhance model performance. The neural decision forest (NDF) \cite{kontschieder2015deep}, which held cutting-edge performance on ImageNet \cite{deng2009imagenet} in 2015, is an ensemble of DTs, each of which is also an instance of ANTs where the whole GoogLeNet architecture \cite{szegedy2015going} (except for the last linear layer) is used as the root transformer, prior to learning tree-structured classifiers with linear routers. \cite{xiao2017ndt} employed a similar approach with a MLP at the root transformer, and is optimised to minimise a differentiable information gain loss. The conditional network proposed in \cite{ioannou2016decision} sparsified CNN architectures by distributing computations on hierarchical structures based on directed acyclic graphs with MLP-based routers, and designed models with the same accuracy with reduced compute cost and number of parameters. However, in all cases, the model architectures are pre-specified and fixed.

% We consider growth of the architecture another key facet of the DT paradigm, and like the SDTs of \cite{irsoy2012soft} we base this decision on validation set error. Later work in this vein introduced budding trees (BTs), in which ``bud nodes'' can switch between being internal and leaf nodes, alleviating the traditional problems associated with purely greedy growth \cite{irsoy2014budding}. \cite{irsoy2018continuously} introduced (a) tunnel networks and (b) budding perceptrons. Tunnel networks add a highway connection \cite{srivastava2015highway} to internal nodes, allowing them to gate between pure residual \cite{he2016deep} and nonlinear operations. Budding perceptrons extend the bud node concept to potentially decompose internal nodes into their own trees, providing a way for the tree to grow on the inside. 

% Finally, we note that SDTs may be applied to more than just regression or classification; \cite{irsoy2016autoencoder} created autoencoder trees, with SDTs used in both the encoder and decoder.
%combining dimensionality reduction with the hierarchical clustering properties of DTs . 

In contrast, ANTs satisfy all criteria in Tab.~\ref{tab:comparison}; they provide a general framework for learning tree-structured models with the capacity of representation learning along each path and within routing functions, and a mechanism for learning its architecture.

\begin{table}
	\caption{\footnotesize Comparison of tree-structured NNs. The first column denotes if each path on the tree is a NN, and the second column denotes if the routers learn features. The last column shows if the method grows an architecture, or uses a pre-specified one.\label{tab:comparison}}
	\small
    \centering
	\begin{tabular}{|l|cc|c|}
		\hline
		\multicolumn{1}{|c}{\textbf{Method}} &  \multicolumn{2}{|c|}{\textbf{Feature learning?}} & \multicolumn{1}{c|}{\textbf{Grown?}}  \\
			& Path & Routers &  \\
		\hline
		SDT \cite{suarez1999globally} &\textcolor{red}{\xmark} & \textcolor{red}{\xmark} & \cmark  \\
		SDT 2 / HME \cite{jordan1994hierarchical} &\textcolor{red}{\xmark} &   \cmark & \textcolor{red}{\xmark} \\
        SDT 3 \cite{irsoy2012soft} &\textcolor{red}{\xmark} & \cmark & \cmark  \\
		SDT 4 \cite{frosst2017distilling} & \textcolor{red}{\xmark} &  \cmark & \textcolor{red}{\xmark} \\
		RDT \cite{leon2015policy} & \textcolor{red}{\xmark} & \cmark & \textcolor{red}{\xmark} \\
        BT \cite{irsoy2014budding} &\textcolor{red}{\xmark} & \cmark & \cmark  \\
		Conv DT \cite{laptev2014convolutional} & \textcolor{red}{\xmark} &  \cmark & \textcolor{red}{\xmark} \\
		NDT \cite{rota2014neural} & \textcolor{red}{\xmark} & \cmark & \cmark  \\
		NDT 2 \cite{xiao2017ndt}  & \cmark  & \cmark & \textcolor{red}{\xmark} \\
		NDF \cite{kontschieder2015deep} & \cmark  & \cmark & \textcolor{red}{\xmark}  \\	
		CNet \cite{ioannou2016decision} & \cmark & \cmark & \textcolor{red}{\xmark}\\
		\textbf{ANT (ours)} & \cmark & \cmark & \cmark\\
		\hline
	\end{tabular}
\end{table}

% Another related strand of work for learning features of data is cascaded forests--—stacks of RFs where the outputs of one RF are fed into subsequent RFs \cite{montillo2011entangled,kontschieder2013geof,zhou2017deepft}. It has been shown how a cascade of DTs can be mapped to NNs with sparse connections \cite{sethi1990entropy}, and more recently \cite{richmond2015mapping} extended the argument to RFs. However, the features in this approach are the intermediate outputs of respective component models, which are not optimised for the target task, and cannot be learned end-to-end, thus limiting its representational quality.

Architecture growth is a key facet of DTs \cite{criminisi2013decision}, and typically performed in a greedy fashion with a termination criteria based on validation set error \cite{suarez1999globally,irsoy2012soft}. Previous works in DT research have made attempts to improve upon this greedy growth strategy. Decision jungles \cite{shotton2013decision} employ a training mechanism to merge partitioned input spaces between different sub-trees, and thus to rectify suboptimal ``splits" made due to the locality of optimisation. \cite{irsoy2014budding} proposes budding trees, which are grown and pruned incrementally based on global optimisation of existing nodes. While our training algorithm, for simplicity, grows the architecture by
greedily choosing the best option between going “deeper” and “splitting” the input space (see Fig. \ref{fig:hierarchy}), it is certainly amenable to these advances. 


\paragraph{Conditional Computation:} in NNs, computation of each sample engages every parameter of the model. In contrast, DTs route each sample to a single path, only activating a small fraction of the model. \cite{bengio2013deep} advocated for this notion of conditional computation to be integrated into NNs, and this has become a topic of growing interest. Rationales for using conditional computation ranges from attaining better capacity-to-computation ratio \cite{bengio2013estimating,davis2013low,bengio2015conditional,Shazeer2017OutrageouslyLN} to adapting the required computation to the difficulty of the input and task \cite{bengio2015conditional,almahairi2016dynamic,Teerapittayanon2016BranchyNetFI,Graves2016AdaptiveCT,Figurnov2017SpatiallyAC,veit2017convolutional}. We view the growth procedure of ANTs as having a similar motivation with the latter---processing raw pixels is suboptimal for computer vision tasks, but we have no reason to believe that the hundreds of convolutional layers in current state-of-the-art architectures \cite{he2016deep,huang2017densely} are necessary either. Growing ANTs adapts the architecture complexity to the dataset as a whole, with routers determining the computation needed on a per-sample basis. 

\paragraph{Neural Architecture Search:} the ANT growing procedure is related to the progressive growing of NNs \cite{fahlman1990cascade,hinton2006fast,xiao2014errordriven,chen2016net2net,srivastava2015highway,lee2017lifelong,cai2018efficient,irsoy2018continuously}, or more broadly, the field of neural architecture search \cite{zoph2016neural,brock2017smash,cortes2017adanet}. This approach, mainly via greedy layerwise training, has historically been one solution to optimising NNs \cite{fahlman1990cascade,hinton2006fast}. However, nowadays it is possible to train NNs in an end-to-end fashion. One area which still uses progressive growing is lifelong learning, in which a model needs to adapt to new tasks while retaining performance on previous ones \cite{xiao2014errordriven,lee2017lifelong}. In particular, \cite{xiao2014errordriven} introduced a method that grows a tree-shaped network to accommodate new classes. However, their method never transforms the data before passing it to the children classifiers, and hence never benefit from the parent's representations. 

Whilst we learn the architecture of an ANT in a greedy, layerwise fashion, several other methods search globally. Based on a variety of techniques, including evolutionary algorithms \cite{stanley2002evolving,real2017large}, reinforcement learning \cite{zoph2016neural}, sequential optimisation \cite{liu2017progressive} and boosting \cite{cortes2017adanet}, these methods find extremely high-performance yet complex architectures. In our case, we constrain the search space to simple tree-structured NNs, retaining desirable properties of DTs such as data-dependent computation and interpretable structures, while keeping the space and time requirement of architecture search tractable thanks to the locality of our growth procedure.

\paragraph{Cascaded trees and forests:} another noteworthy strand of work for feature learning with tree-structured models is cascaded forests---stacks of RFs where the outputs of intermediate models are fed into the subsequent ones \cite{montillo2011entangled,kontschieder2013geof,zhou2017deepft}. It has been shown how a cascade of DTs can be mapped to NNs with sparse connections \cite{sethi1990entropy}, and more recently \cite{richmond2015mapping} extended this argument to RFs. However, the features obtained in this approach are the intermediate outputs of respective component models, which are not optimised for the target task, and cannot be learned end-to-end, thus limiting its representational quality. Recently, \cite{feng2018multi} introduced a method to jointly train a cascade of gradient boosted trees (GBTs) to improve the limited representation learning ability of such previous work. A variant of target propagation \cite{lee2015difference} was designed to enable the end-to-end training of cascaded GBTs, each of which is non-differentiable and thus not amenable to back-propagation. 

% Architecture growth is a key facet of DTs \cite{criminisi2013decision}, and while we grow ANTs in a greedy fashion with a termination criteria based on validation set error \cite{suarez1999globally,irsoy2012soft}, we review here attempts to improve upon this in the DT literature. For example, decision jungles \cite{shotton2013decision} employ a training mechanism to merge partitioned input spaces between different sub-trees, and thus to rectify suboptimal ``splits" made due to the locality of optimisation. \cite{irsoy2014budding} proposes budding trees, which are grown and pruned incrementally based on global optimisation of all existing nodes.

% Another related strand of work for feature learning is cascaded forests---stacks of RFs where the outputs of intermediate models are fed into the subsequent ones \cite{montillo2011entangled,kontschieder2013geof,zhou2017deepft}. It has been shown how a cascade of DTs can be mapped to NNs with sparse connections \cite{sethi1990entropy}, and more recently \cite{richmond2015mapping} extended this argument to RFs. However, the features obtained in this approach are the intermediate outputs of respective component models, which are not optimised for the target task, and cannot be learned end-to-end, thus limiting its representational quality. 

%%%%%%%%%%%%%%%%% RELATED WORK %%%%%%%%%%%%%%%%%%%%%

%Although we focus on single-model performance, ANTs would naturally benefit from ensemble methods such as RFs \cite{breiman2001random} and GBTs \cite{friedman2001greedy}.  A notable modern variant is cascaded forests where RFs are trained in an entangled setting, stacking intermediate classifier outputs with the original input data. Selective examples, such as entangled random forests \cite{montillo2011entangled}, geidesic forests \cite{kontschieder2013geof}, gcForest \cite{zhou2017deepft} have shown to improve the quality of hierarchical learning within constituent DTs. ANTs are amenable to these advancements. 

%It has been shown how a cascade of DTs can be mapped to DNNs with sparse connections \cite{sethi1990entropy}, and more recently \cite{richmond2015mapping} extended the argument to RFs. Our method is ameanable to these advances in
%However, cascaded forests are non-differentiable and cannot be trained in an end-to-end manner.
% We generalise the differentiable soft decision trees proposed in \cite{rota2014neural,kontschieder2015deep,frosst2017distilling} by adding a sequence of non-linear transformations along each path on the tree. Explain that all of these work only aim to learn splitting functions of various complexity with the exception \cite{kontschieder2015deep} which performs a static trasnformation of significant complexity prior to learning the tree-structured hierarchy. 

%(RYU): Moved from method. For example, a standard soft binary decision trees \cite{suarez1999globally} for classification is a specific case where the routers are axis-aligned features, the transformers are identity functions and the solvers are static distribution over classes. The tree model \cite{rota2014neural} instead used multi-layer perceptrons as router modules. Deep Neural Forest \cite{kontschieder2015deep} is an ensemble of neural decision trees, each of which is also an instance of ANT where the whole GoogLeNet architecture except the last fully connected layer is used as the transformer at the root node, and linear classifiers are used for the routers at its internal nodes. However, transformers on the other edges are identity functions and solvers are static class distributions.